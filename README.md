# Multimodal Image Captioning Engine

## Project Overview

This project implements an end-to-end Multimodal AI solution for automatically generating natural language descriptions (captions) from input images. It uses the **Salesforce/blip-image-captioning-base** model via the Hugging Face Transformers library.

## ğŸ§  Architectural Deep Dive

### BLIP Model
The application uses the **BLIP (Bootstrapping Language-Image Pre-training)** model.
-   **Vision Encoder (ViT)**: Processes the image into visual features.
-   **Text Decoder**: Generates the text caption based on the visual features.

## ğŸ› ï¸ Technology Stack

| Category | Technology | Purpose |
| :--- | :--- | :--- |
| **Model** | BLIP Base | Image captioning model. |
| **Frameworks** | Streamlit | Web application interface. |
| **Libraries** | Transformers, Torch | Model loading and inference. |
| **Language** | Python 3.11+ | Primary programming language. |

## ğŸš€ Getting Started

### Prerequisites

Ensure you have Python installed. Install dependencies:

```bash
pip install -r requirements.txt
```

### Running the Application

Run the Streamlit app:

```bash
streamlit run main.py
```
Access at: `http://localhost:8501`

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py             # Main Streamlit application
â”œâ”€â”€ models.py           # BLIP model integration
â”œâ”€â”€ pyproject.toml      # Project dependencies
â””â”€â”€ README.md           # Project documentation
```

## Author
[Your Name]

## License
MIT